{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMP605p9uIwM"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "oCb3cVwzrfbb",
        "outputId": "a9eff381-f7cf-4eb8-ffed-03eca5da6c30"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-989c4e0f48a9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSOURCE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Q3_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "SOURCE_DIR = '/content/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7InRTisGuIwQ"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "86109c09-1b5f-42a8-c5e0-4d51e839e3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading json_lines-0.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "df = pd.read_csv('Q3_data.csv')\n",
        "texts = df['Text'].tolist()\n",
        "# texts[0]\n",
        "\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "for i in range(len(texts)):\n",
        "    texts[i] = delete_hashtag_usernames(texts[i])\n",
        "    texts[i] = delete_url(texts[i])\n",
        "    texts[i] = delete_ex(texts[i])\n",
        "\n",
        "# texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hUbNDpSuIwR"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHJAqBeuIwS"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)¬†\\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3SoSXXHuIwS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "    numerator = np.dot(u, v)\n",
        "    denominator = np.linalg.norm(u) * np.linalg.norm(v)\n",
        "    return numerator / denominator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbh4RPd_uIwS"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vnhYXDQfuIwT"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  similarity_dict = {}\n",
        "  for otherword in embedding_dict:\n",
        "    if otherword != word:\n",
        "      similarity = cosine_similarity(embedding_dict[word],embedding_dict[otherword])\n",
        "      similarity_dict[otherword] = similarity\n",
        "  sorted_similarity_dict = dict(sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "  return dict(list(sorted_similarity_dict.items())[:k])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSA3zfxUFxM9"
      },
      "outputs": [],
      "source": [
        "## one hot encoding manually\n",
        "# embedding_dict = {}\n",
        "# def one_hot_encode(word,vocab):\n",
        "#     word_vector = [0] * len(vocab)\n",
        "#     if word in vocab:\n",
        "#         idx = vocab.index(word)\n",
        "#         word_vector[idx] = 1\n",
        "#         embedding_dict[word] = word_vector\n",
        "#     return embedding_dict\n",
        "\n",
        "\n",
        "# vocabulary = list(set(word for text in texts for word in text.split()))\n",
        "# # one_hot_vectors = [one_hot_encode(word, vocabulary) for word in vocabulary]\n",
        "# for word in vocabulary:\n",
        "#     one_hot_encode(word, vocabulary)\n",
        "\n",
        "\n",
        "# # embedding_dict['ÿ™ŸÑÿßÿ¥ÿ™']\n",
        "# len(embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "92e01137-481e-4fbb-f426-21d226461803"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32115"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. find one hot encoding of each word\n",
        "words = [word for text in texts for word in text.split()]\n",
        "vocabulary = list(set(words))\n",
        "encoder = OneHotEncoder()\n",
        "word_vector = encoder.fit_transform(np.array(vocabulary).reshape(-1, 1)).toarray()\n",
        "embedding_dict = {word: word_vector[i] for i, word in enumerate(vocabulary)}\n",
        "len(embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYT_I1aNuIwT",
        "outputId": "767a775f-2a11-491a-fd06-91692160f812"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ÿÆŸàÿ±ÿß': 0.0,\n",
              " 'ŸÜÿßŸÇÿµ': 0.0,\n",
              " 'ÿßÿ≠ÿ≥ÿßÿ≥€å': 0.0,\n",
              " 'ÿ®ŸàŸâ': 0.0,\n",
              " 'ÿ±ŸÅÿ™Ÿá!': 0.0,\n",
              " 'ŸÜÿ™ŸàÿßŸÜÿ≥ÿ™€åŸÖ': 0.0,\n",
              " 'ŸÜÿ¥€åŸÖ': 0.0,\n",
              " 'ÿ®ŸÅÿ±ŸÖÿß€å€åÿØ....': 0.0,\n",
              " 'ÿ®ÿ±⁄Ü€åÿØŸÜ': 0.0,\n",
              " 'ÿ≤Ÿàÿ±': 0.0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. find 10 nearest words from \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "\n",
        "# vocabulary.index('ÿ¢ÿ≤ÿßÿØ€å')\n",
        "# vocabulary[19379]\n",
        "\n",
        "one_hot_nearest = find_k_nearest_neighbors('ÿ¢ÿ≤ÿßÿØ€å',embedding_dict,10)\n",
        "one_hot_nearest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantage:**<br> 1) simple<br> 2) no implied ordering<br>\n",
        "\n",
        "\n",
        "**Disadvantage:**<br> 1) huge vectors<br> 2) no embedded meaning<br> 3) large execution <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTGo3F0juIwT"
      },
      "source": [
        "##### Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB45V99iHThO"
      },
      "source": [
        "All of the values are 0 . because we use cosine similarity.\n",
        "in the numerator of cosine similarity we use dot function and arrays will multiple into each other index-wise.\n",
        "since we use one hot encoding and each word has only one vector it is completely obvious that dot operation result will be 0. and we can see from the result in the last cell,all of the values of dictionary are 0.\n",
        "for example\n",
        "arr1 = [0,0,0,1]\n",
        "arr2 = [1,0,0,0]\n",
        "dot result = sum([0,0,0,0]) = 0\n",
        "denumerator is always 1 * 1 for two words\n",
        "=> cosine similarity = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJMju0Tiw9YA",
        "outputId": "6f572d09-70f5-47dd-ef76-dbfd668a192f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-8f978fe753d9>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return numerator / denominator\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Ÿáÿ¥ÿ™⁄Ø €åÿßÿØÿ™ ŸÜÿ±Ÿá',\n",
              " 'ŸÖŸÜŸÖ ÿßÿ≤ ÿØ€åÿ±Ÿàÿ≤ ŸÜÿ™ ŸÜÿØÿßÿ¥ÿ™ŸÖ ÿßŸÑÿßŸÜ ÿ™ÿßÿ≤Ÿá ÿßŸàŸÖÿØŸá',\n",
              " 'Ÿáÿ¥ÿ™⁄Ø €åÿßÿØÿ™ ŸÜÿ±Ÿá ÿØ€å⁄ØŸá ü¶ã',\n",
              " 'ÿß€åŸÜ ŸáŸÖŸá ŸÑÿ¥⁄©ÿ± ÿßŸàŸÖÿØŸá - ÿπŸÜÿ¥ ÿØ€å⁄ØŸá ÿØÿ± ÿßŸàŸÖÿØŸá',\n",
              " 'Ÿáÿ¥ÿ™⁄Ø €åÿßÿØÿ™ ŸÜÿ±Ÿá ŸÑÿ∑ŸÅÿß',\n",
              " 'Ÿáÿ¥ÿ™⁄Ø €åÿßÿØÿ™ ŸÜÿ±Ÿá ÿÆŸàÿßŸáÿ±ŸÖüåª',\n",
              " '€åÿßÿØÿ™ ŸÜÿ±Ÿá Ÿáÿ¥ÿ™⁄Ø ÿ®ÿ≤ŸÜ',\n",
              " 'Ÿáÿ¥ÿ™⁄Ø €åÿßÿØÿ™ ÿ±ŸÅÿ™ ÿÆŸàÿßŸáÿ±',\n",
              " 'Ÿáÿ¥ÿ™⁄Ø ÿßŸÜ⁄ØŸÑ€åÿ≥€å €åÿßÿØÿ™ ŸÜÿ±Ÿá',\n",
              " '€åÿßÿØÿ™ ÿ¨ÿßŸàÿØÿßŸÜŸá ÿßÿ≥ÿ™']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "\n",
        "# 1. Find the TF-IDF of all tweets\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# 2. Choose one tweet randomly\n",
        "chosen_tweet_index = random.randint(0, len(texts) - 1)\n",
        "chosen_tweet = tfidf_matrix[chosen_tweet_index]\n",
        "# print(\"Chosen Tweet:\", chosen_tweet)\n",
        "\n",
        "# 3. Find 10 nearest texts from the chosen tweet\n",
        "cosine_similarities = [cosine_similarity(chosen_tweet, tfidf_vector) for tfidf_vector in tfidf_matrix]\n",
        "sorted_similarities = sorted(range(len(cosine_similarities)), key=lambda i: cosine_similarities[i], reverse=True)\n",
        "nearest_tweets = [texts[i] for i in sorted_similarities[1:11]]\n",
        "nearest_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF Manually code\n",
        "i commented this the above code is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TF-IDF code manually\n",
        "# import random\n",
        "# import math\n",
        "\n",
        "# # Function to calculate TF\n",
        "# def calculate_tf(tweet, word):\n",
        "#     words_in_tweet = tweet.split()\n",
        "#     word_count = words_in_tweet.count(word)\n",
        "#     return word_count / len(words_in_tweet)\n",
        "\n",
        "# # Function to calculate IDF\n",
        "# def calculate_idf(word, tweets):\n",
        "#     N = len(tweets)\n",
        "#     word_count = sum(1 for tweet in tweets if word in tweet)\n",
        "#     return math.log10(N / (word_count + 1))\n",
        "\n",
        "# # Function to calculate TF-IDF\n",
        "# def calculate_tfidf(tweet, word, tweets):\n",
        "#     tf = calculate_tf(tweet, word)\n",
        "#     idf = calculate_idf(word, tweets)\n",
        "#     return tf * idf\n",
        "\n",
        "# # Function to calculate cosine similarity\n",
        "# def cosine_similarity(tweet1, tweet2):\n",
        "#     dot_product = sum(a * b for a, b in zip(tweet1, tweet2))\n",
        "#     magnitude_tweet1 = math.sqrt(sum(a ** 2 for a in tweet1))\n",
        "#     magnitude_tweet2 = math.sqrt(sum(b ** 2 for b in tweet2))\n",
        "#     return dot_product / (magnitude_tweet1 * magnitude_tweet2)\n",
        "\n",
        "# # 1. Find the TF-IDF of all tweets\n",
        "# def calculate_tfidf_matrix(texts):\n",
        "#     tfidf_matrix = []\n",
        "#     for tweet in texts:\n",
        "#         tfidf_vector = []\n",
        "#         for word in unique_words:\n",
        "#             tfidf_vector.append(calculate_tfidf(tweet, word, texts))\n",
        "#         tfidf_matrix.append(tfidf_vector)\n",
        "#     return tfidf_matrix\n",
        "\n",
        "# # Sample tweets\n",
        "# # 2. Choose one tweet randomly\n",
        "# chosen_tweet_index = random.randint(0, len(texts) - 1)\n",
        "# chosen_tweet = texts[chosen_tweet_index]\n",
        "# print(\"Chosen Tweet:\", chosen_tweet)\n",
        "\n",
        "# # Get unique words from all tweets\n",
        "# unique_words = set(word for tweet in texts for word in tweet.split())\n",
        "\n",
        "# # 1. Calculate TF-IDF matrix\n",
        "# tfidf_matrix = calculate_tfidf_matrix(texts)\n",
        "\n",
        "# # Convert chosen tweet to TF-IDF vector\n",
        "# chosen_tweet_index = texts.index(chosen_tweet)\n",
        "# chosen_tweet_vector = tfidf_matrix[chosen_tweet_index]\n",
        "\n",
        "# # 3. Find 10 nearest texts from the chosen tweet\n",
        "# cosine_similarities = [cosine_similarity(chosen_tweet_vector, tfidf_vector) for tfidf_vector in tfidf_matrix]\n",
        "# sorted_similarities_indices = sorted(range(len(cosine_similarities)), key=lambda i: cosine_similarities[i], reverse=True)\n",
        "# nearest_tweets = [texts[i] for i in sorted_similarities_indices[1:11]]\n",
        "# nearest_tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXv9Bx9muIwU"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "**Advatages:** <br>\n",
        "Easy to Calculate<br>Identifies Important Terms<br>Contextual Relevance<br>\n",
        "Effective for Information Retrieval\n",
        "\n",
        "\n",
        "**Disadvantages:**<br>\n",
        "Lack of Semantic Understanding<br>Does Not Account for Polysemy<br>Lack of Context and Word Order<br>No Distinction Between Different Types of Documents<br>\n",
        "Bias Towards Rare Terms<br>Poor Performance with Short Texts<br>...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCnxqaVY2zCc",
        "outputId": "0f95104d-fc48-49dc-82a6-8a2bb1db795b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Nearest words from 'ÿ¢ÿ≤ÿßÿØ€å': {'ÿßÿ≤ÿßÿØ€å': 0.99586415, 'ÿ≤ŸÜÿå': 0.9919636, 'ÿ≤ŸÜ': 0.98851025, 'ŸÅÿ±ÿØÿß€å': 0.98805386, 'ÿ≤ŸÜÿØ⁄Ø€åÿå': 0.9877055, 'ÿπÿØÿßŸÑÿ™': 0.9866961, 'ÿ¢ÿ≤ÿßÿØ€åÿå': 0.9862849, 'ÿ≤ŸÜÿØ⁄Ø€å': 0.98521626, 'Ÿàÿ∑ŸÜŸÖ': 0.9848946, 'ŸÖÿ±ÿØÿå': 0.98399013}\n"
          ]
        }
      ],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokenized_texts = [nltk.word_tokenize(tweet) for tweet in texts]\n",
        "model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# 2. find 10 nearest words from \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "# nearest_words = model.wv.most_similar(\"ÿ¢ÿ≤ÿßÿØ€å\", topn=10)\n",
        "embedding_dict = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "nearest_words = find_k_nearest_neighbors(\"ÿ¢ÿ≤ÿßÿØ€å\", embedding_dict, 10)\n",
        "print(\"10 Nearest words from 'ÿ¢ÿ≤ÿßÿØ€å':\", nearest_words)\n",
        "\n",
        "## my test\n",
        "# u = model.wv['ÿ¢ÿ≤ÿßÿØ€å']\n",
        "# v = model.wv['ŸÖ€åŸáŸÜ']\n",
        "# cosine_similarity(u,v) == 0.980075\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkbwwVH_uIwU"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "**Advantages:**<br>\n",
        "this algorithm can find words which are semantically close as we can see in previous cell output.<br>\n",
        "Word2Vec reduces the dimensionality of word representations. Compared to one-hot encoded vectors, which can be extremely large, Word2Vec provides compact and efficient representations\n",
        "\n",
        "**Disadvantages:**<br>\n",
        "it only considers local context but GLoVe consider a global window and contexts.<br>\n",
        "it is not good for OOV.<br>\n",
        "it cannot handle polysemy words correctly.<br>\n",
        "Scaling to new languages requires new embedding matrices.<br>\n",
        "it cannot recognize same words but different shape like:<br>\n",
        "ÿßÿ≤ÿßÿØ€å or ÿåÿ¢ÿ≤ÿßÿØ€å\n",
        "<br>in general : Context Window Limitation , Lack of Subword Information: , Out-of-Vocabulary Words , Fixed Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahS1XEQ0SNIt"
      },
      "source": [
        "#### Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVpxENFrSPvO"
      },
      "source": [
        "this algorithm is very effective for finding nearest words.but as we can see there are some little mistakes as i mentioned earlier in disadvantages part.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgVAEQhyOPxg",
        "outputId": "ee413191-b8b1-4218-d978-7fb7dbf85616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "task_name = \"masked-language-modeling\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Nearest Words from 'ÿ¢ÿ≤ÿßÿØ€å': ['ÿ¢ÿ≤ÿßÿØ€å ÿß€åÿ±ÿßŸÜŸÖ‚ù§Ô∏èü§ûüèº', 'ÿ¢ÿ≤ÿßÿØ€å üñ§', 'ÿßÿ≤ÿßÿØ€å üíö‚ù§üïä', 'ÿ®ÿß€å ÿ¢ÿ≤ÿßÿØ€å', 'ÿ±Ÿáÿß€å€å', 'ÿ¢ÿ≤ÿßÿØ€å€≤€≤', 'ÿ¢ÿ≤ÿßÿØ€å‚Ä¶', 'ÿ¢ÿ≤ÿßÿØ€å ŸÇÿ¥ŸÜ⁄ØŸá', 'ÿ¢ÿ≤ÿßÿØ€å€å', 'ÿ¢ÿ≤ÿßÿØ€å€å€å']\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and masked language model (for fine-tuning)\n",
        "tokenizer = TFBertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to create masked inputs (consider different masking strategies)\n",
        "def create_masked_inputs(text, tokenizer, masking_probability=0.15):\n",
        "  inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  # Randomly mask words with a specific probability\n",
        "  for i in range(len(inputs[\"input_ids\"][0])):\n",
        "    if np.random.rand() < masking_probability:\n",
        "      inputs[\"input_ids\"][0, i] = tokenizer.mask_token_id\n",
        "  return inputs\n",
        "\n",
        "# Prepare masked training data (consider batching for large datasets)\n",
        "masked_inputs = [create_masked_inputs(tweet, tokenizer) for tweet in texts]\n",
        "\n",
        "# Define optimizer and loss function (adapted for TensorFlow)\n",
        "optimizer = AdamW(learning_rate=2e-5)  # Adjust learning rate as needed\n",
        "loss_fn = model.compiled_loss\n",
        "\n",
        "# Early stopping to prevent overfitting (optional)\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
        "\n",
        "# Model checkpoint to save the best model (optional)\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=\"./best_model.h5\", monitor=\"val_loss\", save_best_only=True\n",
        ")\n",
        "\n",
        "# Fine-tuning setup for masked language modeling\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)  # Compile model for training\n",
        "\n",
        "# Train the model (consider using validation data if available)\n",
        "model.fit(\n",
        "    masked_inputs,\n",
        "    epochs=3,  # Adjust training epochs\n",
        "    validation_split=0.1,  # Consider using a validation set\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        ")\n",
        "\n",
        "# Get contextualized embeddings for all tweets\n",
        "embeddings = []\n",
        "for tweet in texts:\n",
        "  inputs = tokenizer(tweet, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(inputs[\"labels\"], outputs.logits)  # Access logits for embeddings\n",
        "  # Extract last hidden state from the CLS token (consider averaging for all tokens)\n",
        "  embedding = outputs.pooler_output.numpy()\n",
        "  embeddings.append(embedding)\n",
        "\n",
        "# Find nearest words to \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "query_word = \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "query_embedding = None\n",
        "word_embeddings = tokenizer.get_vocab().keys()\n",
        "\n",
        "# Calculate cosine similarities for all tweets\n",
        "all_cosine_similarities = []\n",
        "for embedding in embeddings:\n",
        "  query_inputs = tokenizer(query_word, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  with tf.GradientTape() as tape:\n",
        "    query_outputs = model(query_inputs)\n",
        "  query_embedding = query_outputs.pooler_output.numpy()\n",
        "  cosine_similarities = np.dot(embedding, query_embedding) / (\n",
        "      np.linalg.norm(embedding) * np.linalg.norm(query_embedding)\n",
        "  )\n",
        "  all_cosine_similarities.append(cosine_similarities)\n",
        "\n",
        "# Find top 10 words for each tweet (loop to find top 10 across all)\n",
        "for tweet, cosine_similarity in zip(texts, all_cosine_similarities):\n",
        "  # Sort indices of cosine similarities in descending order (most similar first)\n",
        "  sorted_indices = np.argsort(cosine_similarity)[::-1][:10]  # Top 10 most similar\n",
        "  top_10_words = [word_embeddings[i] for i in sorted_indices]\n",
        "  print(\"10 Nearest Words from 'ÿ¢ÿ≤ÿßÿØ€å':\", top_10_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc8hBCnn4cV_",
        "outputId": "465c0e87-cc9e-49f1-8ad9-3731d2a68108"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Nearest Words from 'ÿ¢ÿ≤ÿßÿØ€å': ['ÿ¢ÿ≤ÿßÿØ€å ÿß€åÿ±ÿßŸÜŸÖ‚ù§Ô∏èü§ûüèº', 'ÿ¢ÿ≤ÿßÿØ€å üñ§', 'ÿßÿ≤ÿßÿØ€å üíö‚ù§üïä', 'ÿ®ÿß€å ÿ¢ÿ≤ÿßÿØ€å', 'ÿ±Ÿáÿß€å€å', 'ÿ¢ÿ≤ÿßÿØ€å€≤€≤', 'ÿ¢ÿ≤ÿßÿØ€å‚Ä¶', 'ÿ¢ÿ≤ÿßÿØ€å ŸÇÿ¥ŸÜ⁄ØŸá', 'ÿ¢ÿ≤ÿßÿØ€å€å', 'ÿ¢ÿ≤ÿßÿØ€å€å€å']\n"
          ]
        }
      ],
      "source": [
        "### pretrain output without fine-tuning\n",
        "\n",
        "# 1. fine-tune the model base on all tweets\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = {}\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
        "    word_embeddings[text] = word_embedding\n",
        "\n",
        "# Find 10 nearest words from \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "query = \"ÿ¢ÿ≤ÿßÿØ€å\"\n",
        "query_embedding = model(**tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)).last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
        "nearest_words = find_k_nearest_neighbors(query_embedding, word_embeddings, 10)\n",
        "\n",
        "print(\"10 Nearest Words from 'ÿ¢ÿ≤ÿßÿØ€å':\", nearest_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CeEFW77uIwU"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "**Advantages:** <br>\n",
        "Contextual embeddings capture context-dependent meanings by considering the surrounding words in a sentence.<br>\n",
        " This enables them to represent nuances and polysemy more effectively.<br>\n",
        "Contextual embeddings can be fine-tuned for specific downstream tasks, leading to improved performance on tasks like sentiment analysis, question answering, and named entity recognition.<br>\n",
        "\n",
        "Pre-trained contextual embeddings can be transferred to various tasks, reducing the need for extensive task-specific labeled data.<br>\n",
        "Contextual embeddings tend to have better semantic similarity scores, making them useful for information retrieval and search applications.<br>\n",
        "\n",
        "\n",
        "**Disadvantages:**<br>\n",
        "Training and using contextual embeddings, especially large models like BERT, require significant computational resources and memory.<br>\n",
        "Fine-tuning contextual embeddings demands labeled data, which can be expensive and time-consuming to obtain.<br>\n",
        "Contextual embeddings are often considered black boxes, making it challenging to understand why they produce specific results.<br>\n",
        "Contextual embeddings may not perform well out of the box for specialized domains with limited training data.<br>\n",
        "Contextual embeddings can be sensitive to small input perturbations, affecting their robustness.<br>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
